{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndn1954/CohereRAGEmbedChatbot/blob/main/Cohere_RAG_voicebot_with_whisper_Gardio_ui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cohere RAG chatbot for PDF with speech as input and speech as output"
      ],
      "metadata": {
        "id": "2CORRhkOYiax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install cohere\n",
        "!pip install qdrant-client\n",
        "!pip install gradio\n",
        "!pip install langchain-community\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "hb8snxMFf0OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "import os\n",
        "import langchain\n",
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "F215Q2e2f0V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['COHERE_API_KEY'] =  ''    #Set the Cohere API Key"
      ],
      "metadata": {
        "id": "G7IiDWGYZEvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "_2kWA5KGgpWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"./\"\n",
        "DB_FAISS_PATH = \"./\"\n",
        "\n",
        "loader = DirectoryLoader(DATA_PATH,\n",
        "                             glob='*.pdf',\n",
        "                             loader_cls=PyPDFLoader)\n",
        "\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                                chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                    model_kwargs={'device': 'cpu'})\n",
        "\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "db.save_local(DB_FAISS_PATH)"
      ],
      "metadata": {
        "id": "tlWU55eMgrFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "PnvkOqoxj6H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Prompt template for QA retrieval for each vectorstore\n",
        "\"\"\"\n",
        "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below and nothing else.\n",
        "Helpful answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                        input_variables=['context', 'question'])\n"
      ],
      "metadata": {
        "id": "ObcZOqulj9hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = False\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import Cohere\n",
        "qa = RetrievalQA.from_chain_type(llm=Cohere(model=\"command-nightly\", temperature=0.9),\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=db.as_retriever(),\n",
        "                                 chain_type_kwargs={'prompt': prompt},\n",
        "                                 return_source_documents=True)"
      ],
      "metadata": {
        "id": "3OAquM7gZFPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "           \"question 1 ?\",\n",
        "           \"question 2 ?\"\n",
        "           ]"
      ],
      "metadata": {
        "id": "d2WjZN0OZFX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "# Test if RAG bot is working with cohere\n",
        "for question in questions:\n",
        "  starttime = datetime.datetime.now()\n",
        "  answer = qa({\"query\": question})\n",
        "  print(answer)\n",
        "  endtime = datetime.datetime.now()\n",
        "  print('Time taken for each qstn: ', endtime-starttime)"
      ],
      "metadata": {
        "id": "SGpMr0HmZFmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio app to support voice bot"
      ],
      "metadata": {
        "id": "0YIZhPXCi0kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio torch torchaudio transformers\n",
        "!pip install gTTS"
      ],
      "metadata": {
        "id": "TFFwq11ni6UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "import gtts\n",
        "\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
        "\n",
        "def transcribe(audio):\n",
        "    sr, y = audio\n",
        "    y = y.astype(np.float32)\n",
        "    y /= np.max(np.abs(y))\n",
        "\n",
        "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]  # type: ignore\n",
        "\n",
        "def create_audio(txt):\n",
        "  tts = gtts.gTTS(txt)\n",
        "  # save the audio file\n",
        "  tts.save(\"output.mp3\")\n",
        "  return \"output.mp3\"\n",
        "\n",
        "def capture_qstn(audio):\n",
        "    starttime = datetime.datetime.now()\n",
        "    sr, data = audio\n",
        "    txt = transcribe(audio)\n",
        "    print('qstn:',txt)\n",
        "    ans = qa(txt)\n",
        "    print('ans:',ans)\n",
        "    rag_endtime=datetime.datetime.now()\n",
        "    print('Time taken at the end of RAG: ', rag_endtime-starttime)\n",
        "    audi = create_audio(ans['result'])\n",
        "    endtime = datetime.datetime.now()\n",
        "    print('Time taken for each qstn: ', endtime-starttime)\n",
        "    return audi\n",
        "\n",
        "input_audio = gr.Audio(\n",
        "    sources=[\"microphone\"],\n",
        "    waveform_options=gr.WaveformOptions(\n",
        "        waveform_color=\"#01C6FF\",\n",
        "        waveform_progress_color=\"#0066B4\",\n",
        "        skip_length=2,\n",
        "        show_controls=False,\n",
        "    ),\n",
        ")\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=capture_qstn,\n",
        "    inputs=input_audio,\n",
        "    outputs=\"audio\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "ivVESkaNi6Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6J3mlMr8F8T5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}