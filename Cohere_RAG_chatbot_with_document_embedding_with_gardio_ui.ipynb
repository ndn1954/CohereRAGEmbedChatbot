{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Cohere RAG chatbot with document embedding with gardio ui"
      ],
      "metadata": {
        "id": "2CORRhkOYiax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install cohere\n",
        "!pip install qdrant-client\n",
        "!pip install gradio\n",
        "!pip install langchain-community\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "hb8snxMFf0OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "import os\n",
        "import langchain\n",
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "F215Q2e2f0V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['COHERE_API_KEY'] =  ''    #Set the Cohere API Key"
      ],
      "metadata": {
        "id": "G7IiDWGYZEvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "_2kWA5KGgpWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"./\"\n",
        "DB_FAISS_PATH = \"./\"\n",
        "\n",
        "loader = DirectoryLoader(DATA_PATH,\n",
        "                             glob='*.pdf',\n",
        "                             loader_cls=PyPDFLoader)\n",
        "\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                                chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                    model_kwargs={'device': 'cpu'})\n",
        "\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "db.save_local(DB_FAISS_PATH)"
      ],
      "metadata": {
        "id": "tlWU55eMgrFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "PnvkOqoxj6H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Prompt template for QA retrieval for each vectorstore\n",
        "\"\"\"\n",
        "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below and nothing else.\n",
        "Helpful answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                        input_variables=['context', 'question'])\n"
      ],
      "metadata": {
        "id": "ObcZOqulj9hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = False\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import Cohere\n",
        "qa = RetrievalQA.from_chain_type(llm=Cohere(model=\"command-nightly\", temperature=0.9),\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=db.as_retriever(),\n",
        "                                 chain_type_kwargs={'prompt': prompt},\n",
        "                                 return_source_documents=True)"
      ],
      "metadata": {
        "id": "3OAquM7gZFPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "           \"what are the variants of senior first ?\",\n",
        "           \"pre hospitalization ?\",\n",
        "           \"post hospitalization ?\",\n",
        "           \"Entry age?\"\n",
        "           ]"
      ],
      "metadata": {
        "id": "d2WjZN0OZFX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question in questions:\n",
        "  answer = qa({\"query\": question})\n",
        "  print(answer)"
      ],
      "metadata": {
        "id": "SGpMr0HmZFmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document converse bot"
      ],
      "metadata": {
        "id": "HscytScuZXd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import Cohere\n",
        "llm = Cohere(model = \"command\", temperature=0.9)"
      ],
      "metadata": {
        "id": "jIldeptaZFu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
        "#Setup Chat History\n",
        "#The chat_history variable keeps track of the conversation history,\n",
        "#storing the user queries #and the chatbot's responses\n",
        "#This allows the chatbot to maintain context and provide relevant answers\n",
        "# based on past interactions.\n",
        "chat_history = []\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm = llm,\n",
        "    chain_type = \"stuff\",\n",
        "    memory = ConversationSummaryMemory(llm = llm, memory_key='chat_history', input_key='question', output_key= 'answer', return_messages=True),\n",
        "    retriever = db.as_retriever(),\n",
        "    return_source_documents=False,\n",
        "    combine_docs_chain_kwargs={'prompt': prompt}\n",
        ")"
      ],
      "metadata": {
        "id": "gFZR5DalZF8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def QandA(question):\n",
        "    global chat_history\n",
        "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
        "    chat_history.append((question, result['answer']))\n",
        "    return result['answer']"
      ],
      "metadata": {
        "id": "pqXd9FL3ZmOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DocumentQA Interface\n",
        "import gradio as gr\n",
        "DocQABotUI = gr.Interface(\n",
        "    fn=QandA,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Chatbot\",\n",
        "    description=\"Document QA\"\n",
        ")\n",
        "#Launching the DocQABot\n",
        "DocQABotUI.launch(share=True)"
      ],
      "metadata": {
        "id": "vDNIIkhmZmrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
